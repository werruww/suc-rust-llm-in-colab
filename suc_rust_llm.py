# -*- coding: utf-8 -*-
"""suc_rust_llm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19GGxPkxye4apDqykwYfWB55z-MxhZuQZ
"""







# تنصيب Rust (الإصدار الليلي)
!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y --default-toolchain nightly

import os
os.environ['PATH'] += ":/root/.cargo/bin"

!rustc --version
!cargo --version



!curl -fsSL https://ollama.com/install.sh | sh

!ollama pull gpt-oss:20b

!ollama list









/content/llm/Cargo.toml
 [dependencies]
llm = { version = "1.3.3", features = ["openai", "anthropic", "ollama", "deepseek", "xai", "phind", "google", "groq", "mistral", "Elevenlabs"] }

# Commented out IPython magic to ensure Python compatibility.
# %cd /content

!git clone https://github.com/graniet/llm.git

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/llm

!cargo install llm



!ollama pull gemma3:4b

!ollama list

!llm --provider ollama --model gemma3:4b --temperature 0.7













!llm --provider ollama --model gpt-oss:20b --temperature 0.7

# Start Ollama server in the background
get_ipython().system_raw('ollama serve &')

!ollama list

!ollama run gpt-oss:20b

